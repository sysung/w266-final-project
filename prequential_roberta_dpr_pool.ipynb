{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prequential RoBERTa DPR Pool\n",
    "\n",
    "Within the paper [WatClaimCheck: A new Dataset for Claim Entailment and Inference](https://aclanthology.org/2022.acl-long.92.pdf), they state that the Dense Passage Retrieval model using a pooling.\n",
    "\n",
    "## Prequential\n",
    "\n",
    "The prequential dataset are partitions of 6 months such that it simulates how new topics are treated given new information. (See Section 5.2.1 of the paper)\n",
    "\n",
    "## How do they pool?\n",
    "1. For each claim, all of the sentences are pooled from every associated premise article and ranked using a similarity score.\n",
    "2. Evidence sentences are concatenated in the descending order of similarity score.\n",
    "3. The claim text and evidence sentences are concatenated.\n",
    "4. Resulting text is truncated to maximum sequence length of transformer model to perform claim veracity inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-24 21:11:44.679340: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-10-24 21:11:45.468361: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from helper import download_prequential_dataset\n",
    "from transformers import RobertaConfig, RobertaTokenizer, TFRobertaForSequenceClassification\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Global Variables\n",
    "DATASET_FP = \"./WatClaimCheck_dataset\" # CHANGE TO MATCH LOCAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting fields from metadata\n",
      "Extracting fields from label\n"
     ]
    }
   ],
   "source": [
    "# Retrieve dataset\n",
    "full_pd_df = download_prequential_dataset(DATASET_FP)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33721"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(full_pd_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "654"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_pd_df['review_date'].isnull().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9740"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_pd_df['claim_date'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(full_pd_df[(full_pd_df['review_date'].isnull()) & (full_pd_df['claim_date'].isnull())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set model properties\n",
    "max_seq_len = 512\n",
    "model_checkpoint = \"roberta-base\"\n",
    "learning_rate = 0.00001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFRobertaForSequenceClassification: ['roberta.embeddings.position_ids']\n",
      "- This IS expected if you are initializing TFRobertaForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFRobertaForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights or buffers of the TF 2.0 model TFRobertaForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Get model and set number of labels\n",
    "model_config = RobertaConfig.from_pretrained(model_checkpoint)\n",
    "model_config.num_labels = 3\n",
    "model = TFRobertaForSequenceClassification.from_pretrained(model_checkpoint, config=model_config)\n",
    "\n",
    "# Get Tokenizer\n",
    "tokenizer = RobertaTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "# Get optimizer\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
