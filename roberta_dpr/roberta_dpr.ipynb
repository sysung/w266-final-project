{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identifying Misinformation in Social Media and News Sources\n",
    "\n",
    "This notebook creates a model that can detect misinformation within social media and news outlets. We utilize the WatClaimCheck paper as our initial starting point with slight variation. We highlight how our process works below.\n",
    "\n",
    "TODO:\n",
    "- Add high level overview of how notebook is structured\n",
    "- Add diagrams on how models are used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allow access to parent directory\n",
    "import sys\n",
    "import os\n",
    "parent_path = os.path.abspath(os.path.join(os.getcwd(), os.pardir))\n",
    "sys.path.insert(1, parent_path)\n",
    "\n",
    "from helper import download_dataset, download_article\n",
    "\n",
    "# DPR\n",
    "from transformers import DPRContextEncoderTokenizer, TFDPRContextEncoder\n",
    "from transformers import DPRQuestionEncoderTokenizer, TFDPRQuestionEncoder\n",
    "\n",
    "# RoBERTa\n",
    "from transformers import RobertaTokenizer, TFRobertaForSequenceClassification\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read and Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Global Variables\n",
    "DATASET_FP = \"../WatClaimCheck_dataset\" # CHANGE TO MATCH LOCAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve dataset\n",
    "train_df, valid_df, test_df = download_dataset(DATASET_FP)\n",
    "print(train_df.count())\n",
    "print(valid_df.count())\n",
    "print(test_df.count())\n",
    "# print(f\"Train Row Count: {len(train_df)}\")\n",
    "# print(f\"Valid Row Count: {len(valid_df)}\")\n",
    "# print(f\"Test Row Count:  {len(test_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get review article content\n",
    "train_df['review_article_content'] = train_df['review_article'].apply(lambda x: ' '.join(download_article(DATASET_FP, x)))\n",
    "valid_df['review_article_content'] = valid_df['review_article'].apply(lambda x: ' '.join(download_article(DATASET_FP, x)))\n",
    "test_df['review_article_content'] = test_df['review_article'].apply(lambda x: ' '.join(download_article(DATASET_FP, x)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add question\n",
    "train_df['question'] = train_df['claim'].apply(lambda x: f\"Is the claim \\\"{x}\\\" true, false, or partially true/false?\")\n",
    "valid_df['question'] = valid_df['claim'].apply(lambda x: f\"Is the claim \\\"{x}\\\" true, false, or partially true/false?\")\n",
    "test_df['question'] = test_df['claim'].apply(lambda x: f\"Is the claim \\\"{x}\\\" true, false, or partially true/false?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 0\n",
    "print(train_df['claim'][idx])\n",
    "print(train_df['rating'][idx], train_df['original_rating'][idx])\n",
    "print(train_df['premise_articles'][idx])\n",
    "print(train_df['reviewer_site'][idx])\n",
    "print()\n",
    "print(train_df['review_article_content'][idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DPR Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import tensorflow as tf\n",
    "from datasets import load_dataset\n",
    "from transformers import TFAutoModel, AutoTokenizer\n",
    "\n",
    "# Load dataset\n",
    "dataset = load_dataset(\"my_dataset\") # Replace with your dataset name\n",
    "train_dataset = dataset[\"train\"]\n",
    "val_dataset = dataset[\"validation\"]\n",
    "\n",
    "# Load pretrained DPR model and tokenizer\n",
    "model_name = \"facebook/dpr-question_encoder-single-nq-base\" # Replace with your model name\n",
    "model = TFAutoModel.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Define input and output features\n",
    "input_features = [\"question\", \"passages\"]\n",
    "output_features = [\"relevance\"]\n",
    "\n",
    "# Tokenize and encode the dataset\n",
    "def encode(examples):\n",
    "  inputs = tokenizer(examples[\"question\"], examples[\"passages\"], padding=\"max_length\", truncation=True, return_tensors=\"tf\")\n",
    "  outputs = tf.convert_to_tensor(examples[\"relevance\"], dtype=tf.int32)\n",
    "  return inputs, outputs\n",
    "\n",
    "train_dataset = train_dataset.map(encode, batched=True)\n",
    "val_dataset = val_dataset.map(encode, batched=True)\n",
    "\n",
    "# Create a Keras model that wraps the DPR model\n",
    "class DPRModel(tf.keras.Model):\n",
    "  def __init__(self, model):\n",
    "    super().__init__()\n",
    "    self.model = model\n",
    "  \n",
    "  def call(self, inputs):\n",
    "    # Get the question and passage embeddings from the DPR model\n",
    "    question_embeddings = self.model.question_encoder(inputs[\"input_ids\"][:,0,:], attention_mask=inputs[\"attention_mask\"][:,0,:]).pooler_output\n",
    "    passage_embeddings = self.model.ctx_encoder(inputs[\"input_ids\"][:,1:,:], attention_mask=inputs[\"attention_mask\"][:,1:,:]).pooler_output\n",
    "    \n",
    "    # Compute the dot product similarity between question and passage embeddings\n",
    "    similarity_scores = tf.einsum(\"nd,npd->np\", question_embeddings, passage_embeddings)\n",
    "    \n",
    "    # Return the similarity scores as logits\n",
    "    return similarity_scores\n",
    "\n",
    "# Instantiate the Keras model\n",
    "keras_model = DPRModel(model)\n",
    "\n",
    "# Define the loss function and metrics\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "accuracy = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "\n",
    "# Compile the Keras model\n",
    "keras_model.compile(optimizer=\"adam\", loss=loss, metrics=[accuracy])\n",
    "\n",
    "# Fit the Keras model on the dataset\n",
    "keras_model.fit(train_dataset, validation_data=val_dataset, epochs=3) # Adjust the number of epochs as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dpr_context_encoder_tokenizer = DPRContextEncoderTokenizer.from_pretrained('facebook/dpr-ctx_encoder-single-nq-base')\n",
    "dpr_context_encoder_model = TFDPRContextEncoder.from_pretrained('facebook/dpr-ctx_encoder-single-nq-base')\n",
    "\n",
    "dpr_question_encoder_tokenizer = DPRQuestionEncoderTokenizer.from_pretrained('facebook/dpr-question_encoder-single-nq-base')\n",
    "dpr_question_encoder_model = TFDPRQuestionEncoder.from_pretrained('facebook/dpr-question_encoder-single-nq-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = dpr_context_encoder_tokenizer(\n",
    "    train_df['review_article_content'].to_list(),\n",
    "    return_tensors = 'tf'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dpr_context_encoder_model(tokens).pooler_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RoBERTa Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "claim_max_seq_len = 128\n",
    "sentence_max_Seq_len = 320\n",
    "num_train_examples = 26976 # 26976\n",
    "num_valid_examples = 3372 # 3372\n",
    "checkpoint = 'distilroberta-base'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roberta_tokenizer = RobertaTokenizer.from_pretrained(checkpoint)\n",
    "roberta_model = TFRobertaForSequenceClassification.from_pretrained(checkpoint, num_labels=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "\n",
    "train_inputs = roberta_tokenizer(\n",
    "    train_df['claim'][:num_train_examples].to_list(),\n",
    "    max_length=claim_max_seq_len,\n",
    "    truncation=True,\n",
    "    padding='max_length',\n",
    "    return_tensors='tf'\n",
    ")\n",
    "\n",
    "train_labels = encoder.fit_transform(np.array(train_df['rating']).reshape(-1, 1))\n",
    "\n",
    "valid_inputs = roberta_tokenizer(\n",
    "    valid_df['claim'][:num_valid_examples].to_list(),\n",
    "    max_length=claim_max_seq_len,\n",
    "    truncation=True,\n",
    "    padding='max_length',\n",
    "    return_tensors='tf'\n",
    ")\n",
    "\n",
    "valid_labels = encoder.fit_transform(np.array(valid_df['rating']).reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 100\n",
    "learning_rate = 1e-5\n",
    "\n",
    "input_ids = tf.keras.layers.Input(shape=(claim_max_seq_len,), dtype=tf.int64, name='input_ids_layer')\n",
    "attention_mask = tf.keras.layers.Input(shape=(claim_max_seq_len,), dtype=tf.int64, name='attention_mask_layer')\n",
    "\n",
    "roberta_inputs = {'input_ids': input_ids,\n",
    "                   'attention_mask': attention_mask}\n",
    "\n",
    "roberta_model.trainable = True\n",
    "roberta_outputs = roberta_model(roberta_inputs)\n",
    "\n",
    "model = tf.keras.Model(inputs=[input_ids, attention_mask], outputs=roberta_outputs)\n",
    "\n",
    "model.compile(\n",
    "    optimizer = tf.keras.optimizers.AdamW(learning_rate=learning_rate),\n",
    "    loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "    metrics=[\n",
    "        tf.keras.metrics.CategoricalAccuracy(),\n",
    "        tf.keras.metrics.Precision(),\n",
    "        tf.keras.metrics.Recall(),\n",
    "        tf.keras.metrics.F1Score(average='macro')\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.utils.plot_model(model, show_shapes=True, dpi=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 12\n",
    "epochs = 10\n",
    "checkpoint_path = \"training/cp.ckpt\"\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                                 save_weights_only=True,\n",
    "                                                 verbose=1)\n",
    "\n",
    "model_history = model.fit(\n",
    "    [train_inputs.input_ids, train_inputs.attention_mask], train_labels,\n",
    "    validation_data=([valid_inputs.input_ids, valid_inputs.attention_mask], valid_labels),\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    callbacks=[cp_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create subplots\n",
    "fig, axes = plt.subplots(5, 1, figsize=(7, 15))\n",
    "epochs = range(1, len(model_history.history['loss']) + 1)\n",
    "\n",
    "axes[0].plot(epochs, model_history.history[\"loss\"], 'o-', label='Training loss')\n",
    "axes[0].plot(epochs, model_history.history[\"val_loss\"], 'o-', label='Validation loss')\n",
    "axes[0].set_title('Training and Validation Loss')\n",
    "axes[0].set_xlabel('Epochs')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].legend()\n",
    "\n",
    "axes[1].plot(epochs, model_history.history['categorical_accuracy'], 'o-', label='Training accuracy')\n",
    "axes[1].plot(epochs, model_history.history['val_categorical_accuracy'], 'o-', label='Validation accuracy')\n",
    "axes[1].set_title('Training and Validation Categorical Accuracy')\n",
    "axes[1].set_xlabel('Epochs')\n",
    "axes[1].set_ylabel('Accuracy')\n",
    "axes[1].legend()\n",
    "\n",
    "axes[2].plot(epochs, model_history.history[\"precision\"], 'o-', label='Training precision')\n",
    "axes[2].plot(epochs, model_history.history[\"val_precision\"], 'o-', label='Validation precision')\n",
    "axes[2].set_title('Training and Validation Precision')\n",
    "axes[2].set_xlabel('Epochs')\n",
    "axes[2].set_ylabel('Precision')\n",
    "axes[2].legend()\n",
    "\n",
    "axes[3].plot(epochs, model_history.history[\"recall\"], 'o-', label='Training recall')\n",
    "axes[3].plot(epochs, model_history.history[\"val_recall\"], 'o-', label='Validation recall')\n",
    "axes[3].set_title('Training and Validation Recall')\n",
    "axes[3].set_xlabel('Epochs')\n",
    "axes[3].set_ylabel('Recall')\n",
    "axes[3].legend()\n",
    "\n",
    "axes[4].plot(epochs, model_history.history[\"f1_score\"], 'o-', label='Training Macro F1 Score')\n",
    "axes[4].plot(epochs, model_history.history[\"val_f1_score\"], 'o-', label='Validation Macro F1 Score')\n",
    "axes[4].set_title('Training and Validation F1 Score')\n",
    "axes[4].set_xlabel('Epochs')\n",
    "axes[4].set_ylabel('F1 Score')\n",
    "axes[4].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
